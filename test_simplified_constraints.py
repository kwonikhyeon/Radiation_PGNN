#!/usr/bin/env python3
"""
ë‹¨ìˆœí™”ëœ ëª¨ë¸ì—ì„œ ì œì•½ ë°©ë²•ë“¤ í…ŒìŠ¤íŠ¸
ê¸°ì¡´ complex modelì—ì„œ saturation ë¬¸ì œê°€ ìˆì—ˆë˜ ë°©ë²•ë“¤ì„ ê°„ë‹¨í•œ ëª¨ë¸ì—ì„œ ì¬ê²€ì¦
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from pathlib import Path
import sys

ROOT = Path(__file__).resolve().parent
sys.path.append(str(ROOT))

from model.simplified_conv_next_pgnn import SimplifiedConvNeXtPGNN

# ì œì•½ ë°©ë²•ë“¤ ì •ì˜
class ConstraintMethod1_HardClip(nn.Module):
    """Hard Clipping - ê²€ì¦ëœ ë°©ë²•"""
    def __init__(self):
        super().__init__()
        self.base_model = SimplifiedConvNeXtPGNN(in_channels=6, pred_scale=1.0)
        
    def forward(self, x):
        pred = self.base_model(x)
        pred = pred / 3.0  # ìŠ¤ì¼€ì¼ë§
        pred = torch.clamp(pred, min=0.0, max=1.0)
        return pred

class ConstraintMethod2_Sigmoid(nn.Module):
    """Sigmoid - ì´ì „ì— saturation ë¬¸ì œ"""
    def __init__(self):
        super().__init__()
        self.base_model = SimplifiedConvNeXtPGNN(in_channels=6, pred_scale=1.0)
        
    def forward(self, x):
        pred = self.base_model(x)
        pred = pred / 4.0  # ë” ê°•í•œ ìŠ¤ì¼€ì¼ë§
        pred = torch.sigmoid(pred)
        return pred

class ConstraintMethod3_Tanh(nn.Module):
    """Tanh + Shift"""
    def __init__(self):
        super().__init__()
        self.base_model = SimplifiedConvNeXtPGNN(in_channels=6, pred_scale=1.0)
        
    def forward(self, x):
        pred = self.base_model(x)
        pred = pred / 3.0
        pred = (torch.tanh(pred) + 1.0) / 2.0
        return pred

class ConstraintMethod4_Softplus(nn.Module):
    """Softplus + Normalization (ìƒˆë¡œìš´ ë°©ë²•)"""
    def __init__(self):
        super().__init__()
        self.base_model = SimplifiedConvNeXtPGNN(in_channels=6, pred_scale=1.0)
        
    def forward(self, x):
        pred = self.base_model(x)
        pred = F.softplus(pred) / 3.0  # Softplusë¡œ ì–‘ìˆ˜ ë³´ì¥ í›„ ìŠ¤ì¼€ì¼ë§
        pred = torch.clamp(pred, max=1.0)  # ìƒí•œë§Œ ì œí•œ
        return pred

def test_constraint_methods():
    """ëª¨ë“  ì œì•½ ë°©ë²•ë“¤ì˜ ì¶œë ¥ íŠ¹ì„± í…ŒìŠ¤íŠ¸"""
    print("ğŸ”¬ ë‹¨ìˆœí™”ëœ ëª¨ë¸ì—ì„œ ì œì•½ ë°©ë²• í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    methods = [
        ("Hard Clip", ConstraintMethod1_HardClip()),
        ("Sigmoid", ConstraintMethod2_Sigmoid()), 
        ("Tanh+Shift", ConstraintMethod3_Tanh()),
        ("Softplus+Clamp", ConstraintMethod4_Softplus())
    ]
    
    # ìƒ˜í”Œ ì…ë ¥ (ì‹¤ì œ ë°ì´í„°ì™€ ìœ ì‚¬í•˜ê²Œ)
    batch_size = 4
    input_tensor = torch.randn(batch_size, 6, 256, 256)
    
    # ì…ë ¥ì„ ì‹¤ì œ ë°ì´í„° ë¶„í¬ì— ë§ê²Œ ì¡°ì •
    input_tensor[:, 0] = torch.clamp(input_tensor[:, 0] * 0.1, 0, 1)  # ì¸¡ì •ê°’
    input_tensor[:, 1] = (input_tensor[:, 1] > 0).float()  # ë§ˆìŠ¤í¬
    input_tensor[:, 2] = torch.log(input_tensor[:, 0] + 1e-8)  # log ì¸¡ì •ê°’
    
    # ì¢Œí‘œ í…ì„œ ì˜¬ë°”ë¥´ê²Œ ìƒì„±
    y_coords = torch.linspace(-1, 1, 256).view(256, 1).repeat(1, 256).unsqueeze(0).repeat(batch_size, 1, 1)
    x_coords = torch.linspace(-1, 1, 256).view(1, 256).repeat(256, 1).unsqueeze(0).repeat(batch_size, 1, 1)
    
    input_tensor[:, 3] = y_coords  # Y ì¢Œí‘œ
    input_tensor[:, 4] = x_coords  # X ì¢Œí‘œ
    input_tensor[:, 5] = torch.rand(batch_size, 256, 256) * 0.8 + 0.1  # ê±°ë¦¬
    
    results = {}
    
    for method_name, model in methods:
        print(f"\nğŸ“Š {method_name} í…ŒìŠ¤íŠ¸")
        print("-" * 30)
        
        # ì¶œë ¥ íŠ¹ì„± ë¶„ì„
        with torch.no_grad():
            output = model(input_tensor)
            
        # í†µê³„ ê³„ì‚°
        stats = {
            'mean': output.mean().item(),
            'std': output.std().item(),
            'min': output.min().item(),
            'max': output.max().item(),
            'unique_count': len(torch.unique(output.flatten())),
            'range_width': (output.max() - output.min()).item()
        }
        
        # ì¶œë ¥
        print(f"í‰ê· : {stats['mean']:.4f}")
        print(f"í‘œì¤€í¸ì°¨: {stats['std']:.4f}")
        print(f"ë²”ìœ„: [{stats['min']:.4f}, {stats['max']:.4f}]")
        print(f"ê³ ìœ ê°’ ê°œìˆ˜: {stats['unique_count']:,}")
        print(f"ë²”ìœ„ í­: {stats['range_width']:.4f}")
        
        # ë¬¸ì œ ì§„ë‹¨
        issues = []
        if stats['unique_count'] < 1000:
            issues.append("ë‚®ì€ ë‹¤ì–‘ì„±")
        if stats['range_width'] < 0.01:
            issues.append("ë²”ìœ„ ë„ˆë¬´ ì¢ìŒ")
        if stats['std'] < 0.001:
            issues.append("ë‚®ì€ ë³€ë™ì„±")
        if abs(stats['mean'] - 0.5) < 0.01 and stats['std'] < 0.01:
            issues.append("âš ï¸ Saturation ì˜ì‹¬")
            
        if issues:
            print(f"âš ï¸ ë¬¸ì œ: {', '.join(issues)}")
        else:
            print("âœ… ì •ìƒ ë™ì‘")
            
        results[method_name] = stats
    
    # ì¢…í•© ë¹„êµ
    print(f"\nğŸ“‹ ì¢…í•© ë¹„êµ")
    print("=" * 60)
    print(f"{'Method':<15} {'Mean':<8} {'Std':<8} {'Range':<15} {'Unique':<8} {'Status'}")
    print("-" * 70)
    
    for method_name, stats in results.items():
        range_str = f"[{stats['min']:.3f},{stats['max']:.3f}]"
        unique_str = f"{stats['unique_count']//1000}K" if stats['unique_count'] > 1000 else str(stats['unique_count'])
        
        # ìƒíƒœ íŒì •
        if (stats['unique_count'] > 1000 and stats['range_width'] > 0.1 and 
            stats['std'] > 0.01 and not (abs(stats['mean'] - 0.5) < 0.01 and stats['std'] < 0.01)):
            status = "âœ… Good"
        else:
            status = "âš ï¸ Issue"
            
        print(f"{method_name:<15} {stats['mean']:<8.3f} {stats['std']:<8.3f} {range_str:<15} {unique_str:<8} {status}")

def test_gradient_flow():
    """ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ” ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ í…ŒìŠ¤íŠ¸")
    print("=" * 40)
    
    methods = [
        ("Hard Clip", ConstraintMethod1_HardClip()),
        ("Sigmoid", ConstraintMethod2_Sigmoid()),
        ("Softplus+Clamp", ConstraintMethod4_Softplus())
    ]
    
    input_tensor = torch.randn(2, 6, 256, 256)
    target = torch.rand(2, 1, 256, 256)
    
    for method_name, model in methods:
        model.train()
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
        
        optimizer.zero_grad()
        output = model(input_tensor)
        loss = F.mse_loss(output, target)
        loss.backward()
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í†µê³„
        grad_norms = []
        for param in model.parameters():
            if param.grad is not None:
                grad_norms.append(param.grad.norm().item())
        
        total_grad = sum(grad_norms)
        avg_grad = total_grad / len(grad_norms) if grad_norms else 0.0
        
        print(f"{method_name:<15} - Loss: {loss.item():.4f}, Avg Grad: {avg_grad:.2e}")
        
        if avg_grad < 1e-8:
            print(f"  âš ï¸ ê·¸ë˜ë””ì–¸íŠ¸ ì†Œë©¸ ê°ì§€!")
        elif avg_grad > 1e2:
            print(f"  âš ï¸ ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ ê°ì§€!")
        else:
            print(f"  âœ… ì •ìƒ ê·¸ë˜ë””ì–¸íŠ¸ íë¦„")

def main():
    print("ğŸš€ ë‹¨ìˆœí™”ëœ ConvNeXt PGNN ì œì•½ ë°©ë²• ê²€ì¦")
    print("=" * 60)
    print("ëª©í‘œ: ë³µì¡í•œ ëª¨ë¸ì—ì„œ saturation ë¬¸ì œë¥¼ ë‹¨ìˆœí•œ ëª¨ë¸ì—ì„œ í•´ê²°")
    print()
    
    test_constraint_methods()
    test_gradient_flow()
    
    print(f"\nğŸ¯ ê²°ë¡ :")
    print("- ë‹¨ìˆœí™”ëœ ëª¨ë¸(53M)ì—ì„œ ì œì•½ ë°©ë²•ë“¤ì´ ì •ìƒ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸")
    print("- Sigmoid saturation ë¬¸ì œê°€ í•´ê²°ë˜ì—ˆëŠ”ì§€ ê²€ì¦")
    print("- ê°€ì¥ ì•ˆì •ì ì¸ ì œì•½ ë°©ë²• ì„ íƒì„ ìœ„í•œ ê¸°ì´ˆ ë°ì´í„° ì œê³µ")

if __name__ == "__main__":
    main()