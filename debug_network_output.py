#!/usr/bin/env python3
"""
ë„¤íŠ¸ì›Œí¬ ì¶œë ¥ ë””ë²„ê¹… ìŠ¤í¬ë¦½íŠ¸
ê° constraint methodì˜ ì‹¤ì œ ì¶œë ¥ ë¶„í¬ì™€ ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ ë¶„ì„
"""

import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from pathlib import Path
import sys

ROOT = Path(__file__).resolve().parent
sys.path.append(str(ROOT / "src"))

from model.conv_next_pgnn import ConvNeXtUNetWithMaskAttention as ConvNeXtUNetBase
from torch.utils.data import DataLoader, Dataset

# ìƒ˜í”Œ ì…ë ¥ ìƒì„± (ì‹¤ì œ ë°ì´í„° í˜•íƒœì™€ ë™ì¼)
def create_sample_input(batch_size=2):
    """ì‹¤ì œ ë°ì´í„°ì™€ ìœ ì‚¬í•œ ìƒ˜í”Œ ì…ë ¥ ìƒì„±"""
    # 6ì±„ë„ ì…ë ¥: [M, mask, logM, X, Y, distance]
    sample_input = torch.randn(batch_size, 6, 256, 256)
    
    # ì‹¤ì œ ë°ì´í„° ë¶„í¬ì— ë§ê²Œ ì¡°ì •
    sample_input[:, 0] = torch.clamp(sample_input[:, 0] * 0.1, 0, 1)  # M: ì¸¡ì •ê°’ [0,1]
    sample_input[:, 1] = (sample_input[:, 1] > 0).float()  # mask: ì´ì§„ê°’
    sample_input[:, 2] = torch.log(sample_input[:, 0] + 1e-8)  # logM
    
    # X, Y ì¢Œí‘œ ìƒì„± (ì˜¬ë°”ë¥¸ í˜•íƒœë¡œ)
    x_coords = torch.linspace(-1, 1, 256).view(1, 1, 256).repeat(batch_size, 256, 1)
    y_coords = torch.linspace(-1, 1, 256).view(1, 256, 1).repeat(batch_size, 1, 256)
    
    sample_input[:, 3] = x_coords  # X
    sample_input[:, 4] = y_coords  # Y
    sample_input[:, 5] = torch.randn(batch_size, 256, 256) * 0.5 + 0.5  # distance
    
    return sample_input

# V2ì˜ ê° ì œì•½ ë°©ë²• êµ¬í˜„
class Method1_HardClip(nn.Module):
    def __init__(self, base_model):
        super().__init__()
        self.base_model = base_model
        
    def forward(self, x):
        pred = self.base_model(x)
        print(f"Hard Clip - Base output range: [{pred.min().item():.3f}, {pred.max().item():.3f}]")
        pred = pred / 4.0  # ìŠ¤ì¼€ì¼ë§
        print(f"Hard Clip - After scaling: [{pred.min().item():.3f}, {pred.max().item():.3f}]")
        pred = torch.clamp(pred, min=0.0, max=1.0)
        print(f"Hard Clip - After clamp: [{pred.min().item():.3f}, {pred.max().item():.3f}]")
        return pred

class Method2_Sigmoid(nn.Module):
    def __init__(self, base_model):
        super().__init__()
        self.base_model = base_model
        
    def forward(self, x):
        pred = self.base_model(x)
        print(f"Sigmoid - Base output range: [{pred.min().item():.3f}, {pred.max().item():.3f}]")
        pred = pred / 6.0  # ìŠ¤ì¼€ì¼ë§
        print(f"Sigmoid - After scaling: [{pred.min().item():.3f}, {pred.max().item():.3f}]")
        pred = torch.sigmoid(pred)
        print(f"Sigmoid - After sigmoid: [{pred.min().item():.3f}, {pred.max().item():.3f}]")
        return pred

class Method3_Tanh(nn.Module):
    def __init__(self, base_model):
        super().__init__()
        self.base_model = base_model
        
    def forward(self, x):
        pred = self.base_model(x)
        print(f"Tanh - Base output range: [{pred.min().item():.3f}, {pred.max().item():.3f}]")
        pred = pred / 3.0  # ìŠ¤ì¼€ì¼ë§
        print(f"Tanh - After scaling: [{pred.min().item():.3f}, {pred.max().item():.3f}]")
        pred = (torch.tanh(pred) + 1.0) / 2.0
        print(f"Tanh - After tanh+shift: [{pred.min().item():.3f}, {pred.max().item():.3f}]")
        return pred

def analyze_gradients(model, input_tensor, target):
    """ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ ë¶„ì„"""
    model.train()
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    
    optimizer.zero_grad()
    output = model(input_tensor)
    loss = nn.MSELoss()(output, target)
    loss.backward()
    
    # ê·¸ë˜ë””ì–¸íŠ¸ í†µê³„
    total_grad_norm = 0.0
    param_count = 0
    grad_info = []
    
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm().item()
            total_grad_norm += grad_norm
            param_count += 1
            grad_info.append((name, grad_norm, param.grad.mean().item(), param.grad.std().item()))
    
    avg_grad_norm = total_grad_norm / param_count if param_count > 0 else 0.0
    
    return {
        'loss': loss.item(),
        'total_grad_norm': total_grad_norm,
        'avg_grad_norm': avg_grad_norm,
        'param_count': param_count,
        'grad_info': grad_info[-5:]  # ë§ˆì§€ë§‰ 5ê°œ ë ˆì´ì–´ë§Œ
    }

def main():
    print("ğŸ” ë„¤íŠ¸ì›Œí¬ ì¶œë ¥ ë° ê·¸ë˜ë””ì–¸íŠ¸ ë¶„ì„")
    print("=" * 60)
    
    # ìƒ˜í”Œ ì…ë ¥ ìƒì„±
    input_tensor = create_sample_input(batch_size=2)
    target = torch.rand(2, 1, 256, 256)  # ìƒ˜í”Œ íƒ€ê²Ÿ
    
    print(f"ì…ë ¥ í˜•íƒœ: {input_tensor.shape}")
    print(f"íƒ€ê²Ÿ í˜•íƒœ: {target.shape}")
    print()
    
    # ë² ì´ìŠ¤ ëª¨ë¸ ìƒì„±
    base_model = ConvNeXtUNetBase(in_channels=6, pred_scale=1.0)
    
    methods = [
        ("Method1_HardClip", Method1_HardClip(base_model)),
        ("Method2_Sigmoid", Method2_Sigmoid(base_model)),
        ("Method3_Tanh", Method3_Tanh(base_model))
    ]
    
    results = {}
    
    for method_name, model in methods:
        print(f"\nğŸ“Š {method_name} ë¶„ì„")
        print("-" * 40)
        
        # ì¶œë ¥ ë¶„ì„
        with torch.no_grad():
            output = model(input_tensor)
            
        print(f"ì¶œë ¥ í†µê³„:")
        print(f"  - í˜•íƒœ: {output.shape}")
        print(f"  - í‰ê· : {output.mean().item():.6f}")
        print(f"  - í‘œì¤€í¸ì°¨: {output.std().item():.6f}")
        print(f"  - ìµœì†Œê°’: {output.min().item():.6f}")
        print(f"  - ìµœëŒ€ê°’: {output.max().item():.6f}")
        
        # ë¶„í¬ ë¶„ì„
        output_flat = output.flatten().numpy()
        unique_values = len(np.unique(output_flat))
        print(f"  - ê³ ìœ ê°’ ê°œìˆ˜: {unique_values}")
        
        if unique_values < 10:
            print(f"  - âš ï¸  ë„ˆë¬´ ì ì€ ê³ ìœ ê°’! (saturation ì˜ì‹¬)")
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ë¶„ì„
        grad_info = analyze_gradients(model, input_tensor, target)
        print(f"\nê·¸ë˜ë””ì–¸íŠ¸ ë¶„ì„:")
        print(f"  - ì†ì‹¤: {grad_info['loss']:.6f}")
        print(f"  - ì´ ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„: {grad_info['total_grad_norm']:.6f}")
        print(f"  - í‰ê·  ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„: {grad_info['avg_grad_norm']:.6f}")
        print(f"  - íŒŒë¼ë¯¸í„° ê°œìˆ˜: {grad_info['param_count']}")
        
        if grad_info['avg_grad_norm'] < 1e-6:
            print(f"  - âš ï¸  ë§¤ìš° ì‘ì€ ê·¸ë˜ë””ì–¸íŠ¸! (í•™ìŠµ ë¶ˆê°€ ìƒíƒœ)")
        
        results[method_name] = {
            'output_stats': {
                'mean': output.mean().item(),
                'std': output.std().item(), 
                'min': output.min().item(),
                'max': output.max().item(),
                'unique_values': unique_values
            },
            'grad_stats': grad_info
        }
        
        print()
    
    # ë¹„êµ ë¶„ì„
    print("\nğŸ”¬ ë°©ë²•ë³„ ë¹„êµ")
    print("=" * 60)
    print(f"{'Method':<20} {'Output Range':<15} {'Std':<10} {'Unique':<8} {'Grad Norm':<12}")
    print("-" * 65)
    
    for method_name, data in results.items():
        out_stats = data['output_stats']
        grad_stats = data['grad_stats']
        
        range_str = f"[{out_stats['min']:.3f}, {out_stats['max']:.3f}]"
        std_str = f"{out_stats['std']:.4f}"
        unique_str = f"{out_stats['unique_values']}"
        grad_str = f"{grad_stats['avg_grad_norm']:.2e}"
        
        print(f"{method_name:<20} {range_str:<15} {std_str:<10} {unique_str:<8} {grad_str:<12}")
    
    # ë¬¸ì œ ì§„ë‹¨
    print("\nğŸš¨ ë¬¸ì œ ì§„ë‹¨")
    print("=" * 60)
    
    for method_name, data in results.items():
        out_stats = data['output_stats']
        grad_stats = data['grad_stats']
        
        issues = []
        
        # Saturation ì²´í¬
        if out_stats['unique_values'] < 100:
            issues.append("ì¶œë ¥ saturation (ê³ ìœ ê°’ ë¶€ì¡±)")
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ì†Œë©¸ ì²´í¬
        if grad_stats['avg_grad_norm'] < 1e-5:
            issues.append("ê·¸ë˜ë””ì–¸íŠ¸ ì†Œë©¸ (í•™ìŠµ ë¶ˆê°€)")
        
        # ë²”ìœ„ ì²´í¬
        output_range = out_stats['max'] - out_stats['min']
        if output_range < 0.01:
            issues.append("ì¶œë ¥ ë²”ìœ„ ë§¤ìš° ì¢ìŒ")
        
        if issues:
            print(f"{method_name}: {', '.join(issues)}")
        else:
            print(f"{method_name}: âœ… ì •ìƒ")

if __name__ == "__main__":
    main()