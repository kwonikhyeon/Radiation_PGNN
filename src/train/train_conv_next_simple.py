#!/usr/bin/env python3
"""
ConvNeXt PGNN - SIMPLIFIED TRAINING (ë¬¸ì œ í•´ê²°ìš©)
ë³µì¡í•œ ì†ì‹¤ í•¨ìˆ˜ë“¤ì„ ì œê±°í•˜ê³  í•µì‹¬ë§Œ ë‚¨ê¸´ ì•ˆì •í™”ëœ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
"""
import argparse, json, math, os, random, pathlib, sys
from pathlib import Path

import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm

ROOT = pathlib.Path(__file__).resolve().parents[1]
sys.path.append(str(ROOT))
from model.conv_next_pgnn import (
    ConvNeXtUNetWithMaskAttention as ConvNeXtUNet,
    laplacian_loss,
    physics_loss_unified
)

class RadFieldDataset(Dataset):
    def __init__(self, npz_path: Path):
        self.data = np.load(npz_path)
        if "inp" in self.data:
            self.inp = self.data["inp"].astype(np.float32)
        else:
            chans = [self.data[k].astype(np.float32)
                      for k in ["M", "mask", "logM", "X", "Y", "distance"]]
            self.inp = np.stack(chans, axis=1)
        self.gt  = self.data["gt"].astype(np.float32)
        self.names = [f"{npz_path.stem}_{i}" for i in range(len(self.inp))]

    def __len__(self):  return len(self.inp)

    def __getitem__(self, idx):
        return ( torch.from_numpy(self.inp[idx]),
                 torch.from_numpy(self.gt[idx]),
                 self.names[idx] )

from kornia.metrics import ssim as kornia_ssim

def ssim(pred, gt):
    try:
        return kornia_ssim(pred, gt, window_size=11, reduction="none").mean()
    except TypeError:
        return kornia_ssim(pred, gt, window_size=11).mean()

def train_one_epoch_simple(model, loader, optimizer, epoch, cfg):
    """ë‹¨ìˆœí™”ëœ í›ˆë ¨ í•¨ìˆ˜ - í•µì‹¬ ì†ì‹¤ë§Œ ì‚¬ìš©"""
    model.train()
    losses = []
    ssims = []
    
    pbar = tqdm(loader, desc=f"Epoch {epoch:3d}")
    for batch_idx, (inp, gt, names) in enumerate(pbar):
        inp, gt = inp.to(cfg.device), gt.to(cfg.device)
        
        optimizer.zero_grad()
        
        mask = inp[:, 1:2]  # measurement mask
        pred = model(inp)   # [B, 1, H, W]

        # 1. CORE LOSS: Basic reconstruction with distance weighting
        dist = inp[:, 5:6]
        weight = torch.exp(-2.0 * dist)  # Simple distance weighting
        
        # Unmeasured region loss (í•µì‹¬)
        unmeasured_mask = (1 - mask)
        loss_unmeasured = F.mse_loss(
            pred * unmeasured_mask * weight, 
            gt * unmeasured_mask * weight
        )
        
        # Overall reconstruction loss
        loss_all = F.mse_loss(pred, gt)
        
        # 2. PHYSICS LOSS: Laplacian smoothness (ì ì§„ì  ì ìš©)
        if epoch >= 5:  # 5 ì—í¬í¬ë¶€í„° ì ìš©
            lambda_pg = min(0.05, (epoch - 5) / 20.0 * 0.05)  # ìµœëŒ€ 0.05ê¹Œì§€ ì ì§„ì  ì¦ê°€
            lap_loss = laplacian_loss(pred, mask)
        else:
            lambda_pg = 0.0
            lap_loss = torch.tensor(0.0, device=cfg.device)
        
        # 3. UNIFIED PHYSICS LOSS (ì„ íƒì  ì ìš©)
        if epoch >= 15 and hasattr(cfg, 'use_physics') and cfg.use_physics:
            try:
                physics_loss = physics_loss_unified(pred, gt, inp)
                lambda_physics = min(0.02, (epoch - 15) / 30.0 * 0.02)  # ìµœëŒ€ 0.02
            except:
                physics_loss = torch.tensor(0.0, device=cfg.device)
                lambda_physics = 0.0
        else:
            physics_loss = torch.tensor(0.0, device=cfg.device)
            lambda_physics = 0.0
        
        # 4. MEASUREMENT CONSTRAINT LOSS (gradient-preserving)
        mask_float = mask.float()
        measured_values = inp[:, 0:1]
        measurement_loss = F.mse_loss(pred * mask_float, measured_values * mask_float)
        
        # 5. TOTAL LOSS (ë‹¨ìˆœí™”ë¨ + ì¸¡ì • ì œì•½)
        loss = (
            2.0 * loss_unmeasured +      # ì£¼ìš” ì¬êµ¬ì„± ì†ì‹¤
            0.3 * loss_all +             # ì „ì²´ ì¬êµ¬ì„±
            1000.0 * measurement_loss +  # ê°•í•œ ì¸¡ì •ê°’ ì œì•½ (ê·¸ë˜ë””ì–¸íŠ¸ ë³´ì¡´)
            lambda_pg * lap_loss +       # ë¬¼ë¦¬ í‰í™œì„± (ì ì§„ì )
            lambda_physics * physics_loss # ë¬¼ë¦¬ ë²•ì¹™ (ì„ íƒì )
        )
        
        # ì•ˆì „ ì¥ì¹˜
        if torch.isnan(loss) or torch.isinf(loss) or loss > 50.0:
            print(f"Warning: Abnormal loss {loss:.4f} at epoch {epoch}, batch {batch_idx}")
            continue
        
        loss.backward()
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ì•ˆì •ì„±)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        
        losses.append(loss.item())
        
        # SSIM ê³„ì‚°
        with torch.no_grad():
            ssim_val = ssim(pred, gt).item()
            ssims.append(ssim_val)
        
        # ì§„í–‰ë¥  í‘œì‹œ
        pbar.set_postfix({
            "Loss": f"{loss.item():.4f}",
            "SSIM": f"{ssim_val:.4f}",
            "LapW": f"{lambda_pg:.4f}",
            "PhysW": f"{lambda_physics:.4f}"
        })
    
    return np.mean(losses), np.mean(ssims)

def validate(model, loader, cfg):
    model.eval()
    losses = []
    ssims = []
    
    with torch.no_grad():
        for inp, gt, names in tqdm(loader, desc="Validation"):
            inp, gt = inp.to(cfg.device), gt.to(cfg.device)
            
            pred = model(inp)
            loss = F.mse_loss(pred, gt)
            losses.append(loss.item())
            
            ssim_val = ssim(pred, gt).item()
            ssims.append(ssim_val)
    
    return np.mean(losses), np.mean(ssims)

class SimpleTrainConfig:
    def __init__(self, **kwargs):
        self.epochs = kwargs.get('epochs', 60)
        self.batch_size = kwargs.get('batch_size', 16)
        self.lr = kwargs.get('lr', 2e-4)
        
        # ë‹¨ìˆœí™”ëœ ëª¨ë¸ ì„¤ì •
        self.pred_scale = kwargs.get('pred_scale', 2.0)  # ë³´ìˆ˜ì  ìŠ¤ì¼€ì¼
        
        # ê²½ë¡œ
        self.data_dir = Path(kwargs.get('data_dir', 'data'))
        self.save_dir = Path(kwargs.get('save_dir', 'checkpoints/convnext_simple_fix'))
        
        # ë¬¼ë¦¬ ì†ì‹¤ ì‚¬ìš© ì—¬ë¶€
        self.use_physics = kwargs.get('use_physics', False)  # ê¸°ë³¸ì ìœ¼ë¡œ ë¹„í™œì„±í™”
        
        # ë””ë°”ì´ìŠ¤
        self.device = kwargs.get('device', 'cuda' if torch.cuda.is_available() else 'cpu')

def main():
    parser = argparse.ArgumentParser(description="ConvNeXt PGNN - Simplified Training")
    parser.add_argument("--data_dir", type=str, default="data")
    parser.add_argument("--save_dir", type=str, default="checkpoints/convnext_simple_exp1")
    parser.add_argument("--epochs", type=int, default=60)
    parser.add_argument("--batch", type=int, default=16)
    parser.add_argument("--lr", type=float, default=2e-4)
    parser.add_argument("--pred_scale", type=float, default=2.0)
    parser.add_argument("--use_physics", action='store_true', help="Enable physics loss")
    
    args = parser.parse_args()
    
    cfg = SimpleTrainConfig(
        epochs=args.epochs,
        batch_size=args.batch,
        lr=args.lr,
        pred_scale=args.pred_scale,
        use_physics=args.use_physics,
        data_dir=args.data_dir,
        save_dir=args.save_dir
    )
    
    print(f"ğŸš€ SIMPLIFIED ConvNeXt PGNN Training")
    print(f"ğŸ“ Data: {cfg.data_dir}")
    print(f"ğŸ’¾ Save: {cfg.save_dir}")
    print(f"ğŸ¯ Device: {cfg.device}")
    print(f"âš¡ Pred Scale: {cfg.pred_scale}")
    print(f"ğŸ”¬ Physics Loss: {cfg.use_physics}")
    
    # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    cfg.save_dir.mkdir(parents=True, exist_ok=True)
    
    # ì„¤ì • ì €ì¥
    config_dict = {
        'epochs': cfg.epochs,
        'batch_size': cfg.batch_size,
        'lr': cfg.lr,
        'pred_scale': cfg.pred_scale,
        'use_physics': cfg.use_physics,
        'data_dir': str(cfg.data_dir),
        'save_dir': str(cfg.save_dir)
    }
    
    with open(cfg.save_dir / "config.json", "w") as f:
        json.dump(config_dict, f, indent=2)
    
    # ë°ì´í„° ë¡œë”
    train_ds = RadFieldDataset(cfg.data_dir / "train.npz")
    val_ds = RadFieldDataset(cfg.data_dir / "val.npz")
    
    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=4)
    
    print(f"ğŸ“Š Train: {len(train_ds)}, Val: {len(val_ds)}")
    
    # ë‹¨ìˆœí™”ëœ ëª¨ë¸
    model = ConvNeXtUNet(in_channels=6, pred_scale=cfg.pred_scale).to(cfg.device)
    
    # ë³´ìˆ˜ì  ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=1e-4)
    
    # ê°„ë‹¨í•œ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=cfg.epochs, eta_min=cfg.lr * 0.1
    )
    
    best_val_ssim = 0.0
    best_val_loss = float('inf')
    
    print(f"\nğŸš€ Starting simplified training for {cfg.epochs} epochs...")
    
    for epoch in range(1, cfg.epochs + 1):
        # í›ˆë ¨
        train_loss, train_ssim = train_one_epoch_simple(model, train_loader, optimizer, epoch, cfg)
        
        # ê²€ì¦
        val_loss, val_ssim = validate(model, val_loader, cfg)
        
        # í•™ìŠµë¥  ì—…ë°ì´íŠ¸
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']
        
        print(f"Epoch {epoch:3d}/{cfg.epochs} | "
              f"Train: Loss={train_loss:.4f}, SSIM={train_ssim:.4f} | "
              f"Val: Loss={val_loss:.4f}, SSIM={val_ssim:.4f} | "
              f"LR={current_lr:.2e}")
        
        # ìµœê³  ëª¨ë¸ ì €ì¥
        if val_ssim > best_val_ssim:
            best_val_ssim = val_ssim
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_loss,
                'val_ssim': val_ssim,
                'config': config_dict
            }, cfg.save_dir / "ckpt_best.pth")
            print(f"âœ… New best SSIM: {val_ssim:.4f}")
        
        # ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_loss': val_loss,
            'val_ssim': val_ssim,
            'config': config_dict
        }, cfg.save_dir / "ckpt_last.pth")
        
        # ì¡°ê¸° ì¢…ë£Œ (ê²€ì¦ ì†ì‹¤ì´ ê°œì„ ë˜ì§€ ì•Šì„ ë•Œ)
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
        else:
            patience_counter += 1
            
        # ê²€ì¦ ë©”íŠ¸ë¦­ì´ ë„ˆë¬´ ì˜¤ë˜ ì •ì²´ë˜ë©´ ê²½ê³ 
        if epoch > 10 and val_ssim < 0.1:
            print(f"âš ï¸  Warning: SSIM still very low ({val_ssim:.4f}) at epoch {epoch}")
            if epoch > 20 and val_ssim < 0.05:
                print(f"âŒ Training appears to be failing. Consider debugging the model or data.")
    
    print(f"\nğŸ‰ Training completed!")
    print(f"ğŸ“ˆ Best validation SSIM: {best_val_ssim:.4f}")
    print(f"ğŸ’¾ Models saved to: {cfg.save_dir}")

if __name__ == "__main__":
    main()