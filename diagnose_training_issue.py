#!/usr/bin/env python3
"""
í›ˆë ¨ ë¬¸ì œ ì§„ë‹¨ ë„êµ¬
ëª¨ë¸, ë°ì´í„°, ì†ì‹¤ í•¨ìˆ˜ì˜ ë¬¸ì œì ì„ ì²´ê³„ì ìœ¼ë¡œ ì§„ë‹¨
"""
import numpy as np
import torch
import torch.nn.functional as F
import sys, pathlib
import matplotlib.pyplot as plt

ROOT = pathlib.Path(__file__).resolve().parent
sys.path.append(str(ROOT / "src"))

from model.conv_next_pgnn import ConvNeXtUNetWithMaskAttention
from dataset.dataset_generator import RadiationDataset

def diagnose_data():
    """ë°ì´í„°ì…‹ ì§„ë‹¨"""
    print("ğŸ” DIAGNOSING DATASET...")
    
    try:
        train_ds = RadiationDataset("train")
        val_ds = RadiationDataset("val")
        
        print(f"  ğŸ“Š Train samples: {len(train_ds)}")
        print(f"  ğŸ“Š Val samples: {len(val_ds)}")
        
        # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
        inp, gt = train_ds[0]
        print(f"  ğŸ“ Input shape: {inp.shape}")
        print(f"  ğŸ“ GT shape: {gt.shape}")
        
        # ë°ì´í„° ë²”ìœ„ í™•ì¸
        print(f"  ğŸ“Š Input ranges:")
        for i in range(inp.shape[0]):
            print(f"    Ch{i}: [{inp[i].min():.6f}, {inp[i].max():.6f}]")
        
        print(f"  ğŸ“Š GT range: [{gt.min():.6f}, {gt.max():.6f}]")
        
        # NaN/Inf í™•ì¸
        has_nan_inp = torch.isnan(inp).any()
        has_inf_inp = torch.isinf(inp).any()
        has_nan_gt = torch.isnan(gt).any()
        has_inf_gt = torch.isinf(gt).any()
        
        print(f"  ğŸ” Input NaN: {has_nan_inp}, Inf: {has_inf_inp}")
        print(f"  ğŸ” GT NaN: {has_nan_gt}, Inf: {has_inf_gt}")
        
        if has_nan_inp or has_inf_inp or has_nan_gt or has_inf_gt:
            print("  âŒ FOUND NaN/Inf IN DATA!")
            return False
        
        # ì¸¡ì •ê°’ ì¼ê´€ì„± ì¬í™•ì¸
        mask = inp[1]
        measured_values = inp[0]
        measured_positions = torch.where(mask > 0)
        
        if len(measured_positions[0]) > 0:
            consistency_errors = 0
            for i in range(min(10, len(measured_positions[0]))):
                y, x = measured_positions[0][i], measured_positions[1][i]
                field_val = gt[0, y, x]
                meas_val = measured_values[y, x]
                if abs(field_val - meas_val) > 1e-5:
                    consistency_errors += 1
            
            print(f"  ğŸ¯ Measurement consistency errors: {consistency_errors}/10")
            
            if consistency_errors > 0:
                print("  âŒ MEASUREMENT INCONSISTENCY DETECTED!")
                return False
        
        print("  âœ… Dataset appears healthy")
        return True
        
    except Exception as e:
        print(f"  âŒ Dataset error: {e}")
        return False

def diagnose_model():
    """ëª¨ë¸ ì§„ë‹¨"""
    print("\nğŸ§  DIAGNOSING MODEL...")
    
    try:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        model = ConvNeXtUNetWithMaskAttention(in_channels=6, pred_scale=2.0).to(device)
        
        # ëª¨ë¸ íŒŒë¼ë¯¸í„° í™•ì¸
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"  ğŸ“Š Total parameters: {total_params:,}")
        print(f"  ğŸ“Š Trainable parameters: {trainable_params:,}")
        print(f"  ğŸ’¾ Model size: {total_params * 4 / 1024**2:.1f} MB")
        
        if total_params > 150_000_000:  # 150M íŒŒë¼ë¯¸í„° ì´ìƒ
            print("  âš ï¸  Model is very large, may cause memory issues")
        
        # ë”ë¯¸ ì…ë ¥ìœ¼ë¡œ ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸
        dummy_input = torch.randn(2, 6, 256, 256).to(device)
        
        print(f"  ğŸ”„ Testing forward pass...")
        model.eval()
        with torch.no_grad():
            try:
                output = model(dummy_input)
                print(f"  ğŸ“ Output shape: {output.shape}")
                print(f"  ğŸ“Š Output range: [{output.min():.6f}, {output.max():.6f}]")
                
                # NaN/Inf í™•ì¸
                has_nan = torch.isnan(output).any()
                has_inf = torch.isinf(output).any()
                
                if has_nan or has_inf:
                    print(f"  âŒ Model output contains NaN: {has_nan}, Inf: {has_inf}")
                    return False
                
                print("  âœ… Forward pass successful")
                
            except Exception as e:
                print(f"  âŒ Forward pass failed: {e}")
                return False
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í…ŒìŠ¤íŠ¸
        model.train()
        dummy_target = torch.randn_like(output)
        
        try:
            loss = F.mse_loss(output, dummy_target)
            loss.backward()
            
            # ê·¸ë˜ë””ì–¸íŠ¸ í™•ì¸
            grad_norms = []
            for name, param in model.named_parameters():
                if param.grad is not None:
                    grad_norm = param.grad.norm().item()
                    grad_norms.append(grad_norm)
                    if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():
                        print(f"  âŒ NaN/Inf gradient in {name}")
                        return False
            
            avg_grad_norm = np.mean(grad_norms) if grad_norms else 0
            max_grad_norm = np.max(grad_norms) if grad_norms else 0
            
            print(f"  ğŸ“Š Average gradient norm: {avg_grad_norm:.6f}")
            print(f"  ğŸ“Š Max gradient norm: {max_grad_norm:.6f}")
            
            if max_grad_norm > 100:
                print(f"  âš ï¸  Very large gradients detected, may cause instability")
            elif max_grad_norm < 1e-7:
                print(f"  âš ï¸  Very small gradients detected, may indicate vanishing gradients")
            
            print("  âœ… Gradient computation successful")
            
        except Exception as e:
            print(f"  âŒ Gradient computation failed: {e}")
            return False
        
        return True
        
    except Exception as e:
        print(f"  âŒ Model initialization error: {e}")
        return False

def diagnose_training_step():
    """í›ˆë ¨ ìŠ¤í… ì§„ë‹¨"""
    print("\nğŸ¯ DIAGNOSING TRAINING STEP...")
    
    try:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        model = ConvNeXtUNetWithMaskAttention(in_channels=6, pred_scale=2.0).to(device)
        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
        
        # ì‹¤ì œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
        train_ds = RadiationDataset("train")
        inp, gt = train_ds[0]
        inp = inp.unsqueeze(0).to(device)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        gt = gt.unsqueeze(0).to(device)
        
        print(f"  ğŸ“Š Batch input range: [{inp.min():.6f}, {inp.max():.6f}]")
        print(f"  ğŸ“Š Batch GT range: [{gt.min():.6f}, {gt.max():.6f}]")
        
        model.train()
        
        # ì—¬ëŸ¬ ìŠ¤í… ì‹¤í–‰í•˜ì—¬ ì†ì‹¤ ë³€í™” í™•ì¸
        losses = []
        for step in range(5):
            optimizer.zero_grad()
            
            pred = model(inp)
            
            # ë‹¨ìˆœí•œ MSE ì†ì‹¤ë§Œ ì‚¬ìš©
            loss = F.mse_loss(pred, gt)
            
            print(f"  Step {step}: Loss = {loss.item():.6f}, Pred range = [{pred.min():.6f}, {pred.max():.6f}]")
            
            loss.backward()
            
            # ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„ í™•ì¸
            total_grad_norm = 0
            for param in model.parameters():
                if param.grad is not None:
                    total_grad_norm += param.grad.norm().item() ** 2
            total_grad_norm = total_grad_norm ** 0.5
            
            print(f"    Gradient norm: {total_grad_norm:.6f}")
            
            optimizer.step()
            losses.append(loss.item())
        
        # ì†ì‹¤ ë³€í™” ë¶„ì„
        loss_changes = [losses[i+1] - losses[i] for i in range(len(losses)-1)]
        print(f"  ğŸ“ˆ Loss changes: {loss_changes}")
        
        if all(change >= 0 for change in loss_changes):
            print("  âŒ Loss is not decreasing - optimization problem!")
            return False
        elif all(abs(change) < 1e-8 for change in loss_changes):
            print("  âŒ Loss is not changing - no learning!")
            return False
        else:
            print("  âœ… Loss is decreasing - training step working")
            return True
            
    except Exception as e:
        print(f"  âŒ Training step error: {e}")
        return False

def diagnose_measurement_preservation():
    """ì¸¡ì •ê°’ ë³´ì¡´ ë©”ì»¤ë‹ˆì¦˜ ì§„ë‹¨"""
    print("\nğŸ¯ DIAGNOSING MEASUREMENT PRESERVATION...")
    
    try:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        model = ConvNeXtUNetWithMaskAttention(in_channels=6, pred_scale=2.0).to(device)
        
        # ì‹¤ì œ ë°ì´í„° ì‚¬ìš©
        train_ds = RadiationDataset("train")
        inp, gt = train_ds[0]
        inp = inp.unsqueeze(0).to(device)
        gt = gt.unsqueeze(0).to(device)
        
        mask = inp[:, 1:2]  # ì¸¡ì • ë§ˆìŠ¤í¬
        measured_values = inp[:, 0:1]  # ì¸¡ì •ê°’
        
        model.eval()
        with torch.no_grad():
            pred = model(inp)
        
        # ì¸¡ì • ìœ„ì¹˜ì—ì„œ ê°’ í™•ì¸
        measured_positions = torch.where(mask[0, 0] > 0)
        
        if len(measured_positions[0]) > 0:
            preservation_errors = 0
            max_error = 0.0
            
            for i in range(min(10, len(measured_positions[0]))):
                y, x = measured_positions[0][i], measured_positions[1][i]
                
                gt_val = gt[0, 0, y, x].item()
                pred_val = pred[0, 0, y, x].item()
                meas_val = measured_values[0, 0, y, x].item()
                
                error_gt_pred = abs(gt_val - pred_val)
                error_meas_pred = abs(meas_val - pred_val)
                
                max_error = max(max_error, error_meas_pred)
                
                if error_meas_pred > 1e-4:  # í—ˆìš© ì˜¤ì°¨
                    preservation_errors += 1
                
                if i < 3:  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
                    print(f"    Pos ({y:3d},{x:3d}): GT={gt_val:.6f}, Pred={pred_val:.6f}, Meas={meas_val:.6f}")
            
            print(f"  ğŸ“Š Measurement preservation errors: {preservation_errors}/10")
            print(f"  ğŸ“Š Max preservation error: {max_error:.6f}")
            
            if preservation_errors > 3:
                print("  âŒ Measurement preservation is failing!")
                return False
            else:
                print("  âœ… Measurement preservation working")
                return True
        else:
            print("  âš ï¸  No measured positions found")
            return True
            
    except Exception as e:
        print(f"  âŒ Measurement preservation check error: {e}")
        return False

def create_diagnostic_plot():
    """ì§„ë‹¨ ê²°ê³¼ ì‹œê°í™”"""
    print("\nğŸ“Š CREATING DIAGNOSTIC VISUALIZATION...")
    
    try:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        model = ConvNeXtUNetWithMaskAttention(in_channels=6, pred_scale=2.0).to(device)
        
        train_ds = RadiationDataset("train")
        inp, gt = train_ds[0]
        inp_batch = inp.unsqueeze(0).to(device)
        gt_batch = gt.unsqueeze(0).to(device)
        
        model.eval()
        with torch.no_grad():
            pred = model(inp_batch)
        
        # CPUë¡œ ì´ë™
        inp_np = inp.numpy()
        gt_np = gt.numpy()
        pred_np = pred[0].cpu().numpy()
        
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        fig.suptitle('Training Issue Diagnostic Visualization', fontsize=16)
        
        # 1. GT
        im1 = axes[0, 0].imshow(gt_np[0], cmap='hot', origin='upper')
        axes[0, 0].set_title('Ground Truth')
        plt.colorbar(im1, ax=axes[0, 0], shrink=0.8)
        
        # 2. ì´ˆê¸° ì˜ˆì¸¡ (í›ˆë ¨ë˜ì§€ ì•Šì€ ëª¨ë¸)
        im2 = axes[0, 1].imshow(pred_np[0], cmap='hot', origin='upper')
        axes[0, 1].set_title('Model Prediction (Untrained)')
        plt.colorbar(im2, ax=axes[0, 1], shrink=0.8)
        
        # 3. ì¸¡ì • ë§ˆìŠ¤í¬
        mask_np = inp_np[1]
        im3 = axes[0, 2].imshow(mask_np, cmap='gray', origin='upper')
        axes[0, 2].set_title('Measurement Mask')
        plt.colorbar(im3, ax=axes[0, 2], shrink=0.8)
        
        # 4. ì¸¡ì •ê°’
        measured_np = inp_np[0]
        im4 = axes[1, 0].imshow(measured_np, cmap='hot', origin='upper')
        axes[1, 0].set_title('Measured Values')
        plt.colorbar(im4, ax=axes[1, 0], shrink=0.8)
        
        # 5. ì°¨ì´ ë§µ
        diff_np = np.abs(gt_np[0] - pred_np[0])
        im5 = axes[1, 1].imshow(diff_np, cmap='plasma', origin='upper')
        axes[1, 1].set_title('|GT - Prediction| Difference')
        plt.colorbar(im5, ax=axes[1, 1], shrink=0.8)
        
        # 6. íˆìŠ¤í† ê·¸ë¨ ë¹„êµ
        axes[1, 2].hist(gt_np[0].flatten(), bins=50, alpha=0.7, label='GT', density=True)
        axes[1, 2].hist(pred_np[0].flatten(), bins=50, alpha=0.7, label='Pred', density=True)
        axes[1, 2].set_title('Value Distribution')
        axes[1, 2].legend()
        axes[1, 2].set_xlabel('Value')
        axes[1, 2].set_ylabel('Density')
        
        plt.tight_layout()
        plt.savefig('training_diagnostic.png', dpi=150, bbox_inches='tight')
        print("  ğŸ’¾ Saved diagnostic plot as 'training_diagnostic.png'")
        
    except Exception as e:
        print(f"  âŒ Visualization error: {e}")

def main():
    """ë©”ì¸ ì§„ë‹¨ í”„ë¡œì„¸ìŠ¤"""
    print("ğŸš¨ COMPREHENSIVE TRAINING ISSUE DIAGNOSIS")
    print("=" * 60)
    
    results = []
    
    # 1. ë°ì´í„° ì§„ë‹¨
    data_ok = diagnose_data()
    results.append(("Data", data_ok))
    
    # 2. ëª¨ë¸ ì§„ë‹¨
    model_ok = diagnose_model()
    results.append(("Model", model_ok))
    
    # 3. í›ˆë ¨ ìŠ¤í… ì§„ë‹¨
    training_ok = diagnose_training_step()
    results.append(("Training Step", training_ok))
    
    # 4. ì¸¡ì •ê°’ ë³´ì¡´ ì§„ë‹¨
    preservation_ok = diagnose_measurement_preservation()
    results.append(("Measurement Preservation", preservation_ok))
    
    # 5. ì‹œê°í™” ìƒì„±
    create_diagnostic_plot()
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“‹ DIAGNOSIS SUMMARY:")
    
    all_passed = True
    for component, status in results:
        status_icon = "âœ…" if status else "âŒ"
        print(f"  {status_icon} {component}: {'PASS' if status else 'FAIL'}")
        if not status:
            all_passed = False
    
    print("\nğŸ¯ RECOMMENDATIONS:")
    if not all_passed:
        print("  1. Use the simplified training script: train_conv_next_simple.py")
        print("  2. Start with basic MSE loss only")
        print("  3. Check for gradient clipping and learning rate")
        print("  4. Monitor for NaN/Inf values during training")
    else:
        print("  All components appear healthy. The issue may be in loss function complexity.")
        print("  Try the simplified training approach.")

if __name__ == "__main__":
    main()